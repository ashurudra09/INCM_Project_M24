{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Traditional methods of classification:\n",
    "### Setup:"
   ],
   "id": "69db19b04be570d6"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Function which acts as classification function:",
   "id": "7e4ec1c6f76d05be"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def traditional_classifiers(X, y):\n",
    "    y = pd.factorize(y)[0]  # This assigns unique integers to each species\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # 1. Logistic Regression\n",
    "    logistic_model = LogisticRegression(max_iter=200)\n",
    "    logistic_model.fit(X_train_scaled, y_train)\n",
    "    y_pred_logistic = logistic_model.predict(X_test_scaled)\n",
    "    logistic_accuracy = accuracy_score(y_test, y_pred_logistic)\n",
    "    print(f\"Logistic Regression Accuracy: {logistic_accuracy:.2f}\")\n",
    "\n",
    "    # 2. k-Nearest Neighbors (k-NN)\n",
    "    knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "    knn_model.fit(X_train, y_train)\n",
    "    y_pred_knn = knn_model.predict(X_test)\n",
    "    knn_accuracy = accuracy_score(y_test, y_pred_knn)\n",
    "    print(f\"k-NN Accuracy: {knn_accuracy:.2f}\")\n",
    "\n",
    "    # 3. Support Vector Machine (SVM)\n",
    "    svm_model = SVC(kernel='linear')\n",
    "    svm_model.fit(X_train_scaled, y_train)\n",
    "    y_pred_svm = svm_model.predict(X_test_scaled)\n",
    "    svm_accuracy = accuracy_score(y_test, y_pred_svm)\n",
    "    print(f\"SVM Accuracy: {svm_accuracy:.2f}\")\n",
    "\n",
    "    # 4. Decision Tree Classifier\n",
    "    tree_model = DecisionTreeClassifier(random_state=42)\n",
    "    tree_model.fit(X_train, y_train)\n",
    "    y_pred_tree = tree_model.predict(X_test)\n",
    "    tree_accuracy = accuracy_score(y_test, y_pred_tree)\n",
    "    print(f\"Decision Tree Accuracy: {tree_accuracy:.2f}\")\n",
    "\n",
    "    # 5. Random Forest Classifier\n",
    "    forest_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    forest_model.fit(X_train, y_train)\n",
    "    y_pred_forest = forest_model.predict(X_test)\n",
    "    forest_accuracy = accuracy_score(y_test, y_pred_forest)\n",
    "    print(f\"Random Forest Accuracy: {forest_accuracy:.2f}\")\n",
    "    print()\n"
   ],
   "id": "37c94128f4d14d15"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Classification accuracies for Iris dataset:",
   "id": "dd93ab4ad203a0b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load data from CSV file\n",
    "df = pd.read_csv('../datasets/iris.csv')\n",
    "\n",
    "# Separate features and target variable\n",
    "features = df[['sepal_length', 'sepal_width', 'petal_length', 'petal_width']]\n",
    "target = df['species']\n",
    "\n",
    "print(\"Dataset: iris.csv\")\n",
    "traditional_classifiers(features, target)"
   ],
   "id": "e07306667e3804b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Classification accuracies for Penguins dataset:",
   "id": "70698702d5eb6bb4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Load data from CSV file\n",
    "df = pd.read_csv('../datasets/penguins.csv')\n",
    "# Handle missing values by filling NaNs with the mean of each column\n",
    "numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "df[numeric_columns] = df[numeric_columns].fillna(df[numeric_columns].mean())\n",
    "\n",
    "# Separate features and target variable\n",
    "features = df[['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']]\n",
    "target = df['species']\n",
    "\n",
    "print(\"Dataset: penguins.csv\")\n",
    "traditional_classifiers(features, target)"
   ],
   "id": "2fd099ada949db21"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "## Neuron Models:\n",
    "### Setup:"
   ],
   "id": "f1347c2ffb57f54e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from scipy.optimize import differential_evolution\n",
    "import random\n",
    "from random import seed\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def simulate_neuron(a, b, c, d, I, dt=0.5):\n",
    "    \"\"\"Simulate Izhikevich neuron with given parameters and input current\"\"\"\n",
    "    time = np.arange(0, len(I) * dt, dt)\n",
    "    seed(9)\n",
    "    r = random.random()\n",
    "\n",
    "    # Random fluctuation corresponding to excitatory neurons\n",
    "    c, d = c + 15 * r ** 2, d - 6 * r ** 2\n",
    "    v, u = -65, b * (-65)\n",
    "\n",
    "    v_arr, u_arr = [], []\n",
    "    spike_count = 0\n",
    "\n",
    "    # Simulation loop\n",
    "    for t_idx, t in enumerate(time):\n",
    "        current_I = I[t_idx]\n",
    "\n",
    "        v += dt * (0.04 * v ** 2 + 5 * v + 140 - u + current_I)\n",
    "        u += dt * a * (b * v - u)\n",
    "        if v >= 30:  # spike condition\n",
    "            v = c  # reset membrane potential\n",
    "            u += d  # reset recovery variable\n",
    "            spike_count += 1\n",
    "\n",
    "        v_arr.append(v)\n",
    "        u_arr.append(u)\n",
    "\n",
    "    return np.array(v_arr), np.array(u_arr), spike_count"
   ],
   "id": "a8937c011db756d4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Conversion of Data into input signals:",
   "id": "635cd8b629078afb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def preprocess_data(df):\n",
    "    \"\"\"Preprocess penguin dataset by handling categorical variables and missing values\"\"\"\n",
    "    # Make a copy to avoid modifying original data\n",
    "    df = df.copy()\n",
    "\n",
    "    # Convert all column names to lowercase for consistency\n",
    "    df.columns = df.columns.str.lower()\n",
    "\n",
    "    # Handle categorical variables\n",
    "    categorical_columns = df.select_dtypes(include=['object']).columns\n",
    "    for col in categorical_columns:\n",
    "        if col != df.columns[-1]:  # Don't encode the target variable yet\n",
    "            le = LabelEncoder()\n",
    "            df[col] = df[col].fillna('missing')  # Handle NaN in categorical variables\n",
    "            df[col] = le.fit_transform(df[col])\n",
    "\n",
    "    # Separate features and target\n",
    "    X = df.iloc[:, :-1]  # All columns except the last one\n",
    "    y = df.iloc[:, -1]  # Last column is the target\n",
    "\n",
    "    # Handle missing values in numeric features using mean imputation\n",
    "    numeric_imputer = SimpleImputer(strategy='mean')\n",
    "    X = pd.DataFrame(numeric_imputer.fit_transform(X), columns=X.columns)\n",
    "\n",
    "    # Scale features\n",
    "    scaler = MinMaxScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "\n",
    "    # Encode target variable\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y)\n",
    "\n",
    "    return X, y\n",
    "\n",
    "def create_current_signal(row_data, signal_length=100):\n",
    "    \"\"\"Convert a row of iris features into a current signal\"\"\"\n",
    "    # Ensure input is numeric and finite\n",
    "    row_data = np.array(row_data, dtype=float)\n",
    "    row_data = np.nan_to_num(row_data, nan=0.0, posinf=1.0, neginf=-1.0)\n",
    "\n",
    "    signal = np.zeros(signal_length)\n",
    "    segment_length = signal_length // len(row_data)  # Adjust for different number of features\n",
    "\n",
    "    for i in range(len(row_data)):\n",
    "        start_idx = i * segment_length\n",
    "        end_idx = (i + 1) * segment_length if i < len(row_data) - 1 else signal_length\n",
    "        signal[start_idx:end_idx] = row_data[i] * 100\n",
    "\n",
    "    return signal"
   ],
   "id": "6a22113c094f1a5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Function which trains and tests neuron models:",
   "id": "6bbff32863369f0d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def extract_features(v_arr, u_arr, spike_count):\n",
    "    \"\"\"Extract relevant features from neuron simulation\"\"\"\n",
    "    features = [\n",
    "        np.mean(v_arr),  # Mean membrane potential\n",
    "        np.std(v_arr),  # Standard deviation of membrane potential\n",
    "        np.mean(u_arr),  # Mean recovery variable\n",
    "        np.std(u_arr),  # Standard deviation of recovery variable\n",
    "        spike_count  # Number of spikes\n",
    "    ]\n",
    "    return np.array(features)\n",
    "\n",
    "def classify_sample(features):\n",
    "    \"\"\"Simple classifier based on extracted features\"\"\"\n",
    "    # Use spike count as primary classification feature\n",
    "    if features[4] < 10:\n",
    "        return 0\n",
    "    elif features[4] < 20:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "\n",
    "def evaluate_parameters(params, X_train, y_train):\n",
    "    \"\"\"Evaluate a set of neuron parameters\"\"\"\n",
    "    a, b, c, d = params\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for i in range(len(X_train)):\n",
    "        current_signal = create_current_signal(X_train[i])\n",
    "        v_arr, u_arr, spike_count = simulate_neuron(a, b, c, d, current_signal)\n",
    "        features = extract_features(v_arr, u_arr, spike_count)\n",
    "        prediction = classify_sample(features)\n",
    "\n",
    "        if prediction == y_train[i]:\n",
    "            correct_predictions += 1\n",
    "\n",
    "    accuracy = correct_predictions / len(X_train)\n",
    "    return -accuracy  # Negative because we want to maximize accuracy\n",
    "\n",
    "def train_test_wrapper(filename):\n",
    "    # Load data\n",
    "    df = pd.read_csv(filename)\n",
    "    # Preprocess data\n",
    "    print(\"\\nPreprocessing data...\")\n",
    "    X, y = preprocess_data(df)\n",
    "    print(f\"Preprocessed feature shape: {X.shape}\")\n",
    "    print(f\"Number of classes: {len(np.unique(y))}\")\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    print(f\"\\nSplit sizes - Train: {len(X_train)}, Test: {len(X_test)}\")\n",
    "\n",
    "    # Define parameter bounds\n",
    "    bounds = [\n",
    "        (0.001, 0.2),  # a\n",
    "        (0.1, 1),  # b\n",
    "        (-80, -30),  # c\n",
    "        (0, 10)  # d\n",
    "    ]\n",
    "\n",
    "    print(\"\\nOptimizing neuron parameters...\")\n",
    "    # Optimize parameters using differential evolution\n",
    "    result = differential_evolution(\n",
    "        evaluate_parameters,\n",
    "        bounds,\n",
    "        args=(X_train, y_train),\n",
    "        maxiter=50,\n",
    "        popsize=20,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    # Get optimal parameters\n",
    "    a_opt, b_opt, c_opt, d_opt = result.x\n",
    "    print(f\"\\nOptimal parameters: a={a_opt:.3f}, b={b_opt:.3f},\"\n",
    "          f\" c={c_opt:.3f}, d={d_opt:.3f}\")\n",
    "\n",
    "    # Evaluate on test set\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    correct_predictions = 0\n",
    "    for i in range(len(X_test)):\n",
    "        current_signal = create_current_signal(X_test[i])\n",
    "        v_arr, u_arr, spike_count = simulate_neuron(a_opt, b_opt, c_opt, d_opt, current_signal)\n",
    "        features = extract_features(v_arr, u_arr, spike_count)\n",
    "        prediction = classify_sample(features)\n",
    "\n",
    "        if prediction == y_test[i]:\n",
    "            correct_predictions += 1\n",
    "\n",
    "    test_accuracy = correct_predictions / len(X_test)\n",
    "    print(f\"\\nTest accuracy: {test_accuracy:.3f}\")\n",
    "\n",
    "    # Find one sample from each class\n",
    "    class_samples = {}\n",
    "    for idx, label in enumerate(y_test):\n",
    "        if label not in class_samples and len(class_samples) < 3:\n",
    "            class_samples[label] = idx\n",
    "        if len(class_samples) == 3:\n",
    "            break\n",
    "\n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    # Plot settings\n",
    "    colors = ['#2ecc71', '#3498db', '#e74c3c']\n",
    "    class_names = ['Class 0', 'Class 1', 'Class 2']\n",
    "\n",
    "    # Create subplots for each class\n",
    "    for i, (label, idx) in enumerate(class_samples.items()):\n",
    "        # Generate signals for this sample\n",
    "        current_signal = create_current_signal(X_test[idx])\n",
    "        v_arr, u_arr, spike_count = simulate_neuron(a_opt, b_opt, c_opt, d_opt, current_signal)\n",
    "\n",
    "        # Plot current signal\n",
    "        plt.subplot(3, 2, 2 * i + 1)\n",
    "        plt.plot(current_signal, color=colors[i], linewidth=2)\n",
    "        plt.title(f'{class_names[i]} - Input Current Signal')\n",
    "        plt.xlabel('Time step')\n",
    "        plt.ylabel('Current (I)')\n",
    "\n",
    "        # Plot membrane potential\n",
    "        plt.subplot(3, 2, 2 * i + 2)\n",
    "        plt.plot(v_arr, color=colors[i], linewidth=2)\n",
    "        plt.title(f'{class_names[i]} - Membrane Potential (Spikes: {spike_count})')\n",
    "        plt.xlabel('Time step')\n",
    "        plt.ylabel('Voltage (mV)')\n",
    "\n",
    "        # Add spike count annotation\n",
    "        plt.text(0.02, 0.98, f'Spike Count: {spike_count}',\n",
    "                 transform=plt.gca().transAxes,\n",
    "                 bbox=dict(facecolor='white', alpha=0.8),\n",
    "                 verticalalignment='top')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "id": "f25b8d5fb4b23458"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Accuracies for Iris dataset:",
   "id": "c7dc46f01055f43d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Dataset: iris.csv\")\n",
    "train_test_wrapper('../datasets/iris.csv')"
   ],
   "id": "69cc9af284d0d870"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Accuracies for Penguins dataset:",
   "id": "6695a64a8bc53b87"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"Dataset: penguins.csv\")\n",
    "train_test_wrapper('../datasets/penguins.csv')"
   ],
   "id": "71a2cb6d2e438a28"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
